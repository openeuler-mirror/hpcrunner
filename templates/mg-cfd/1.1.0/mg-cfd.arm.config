[SERVER]
11.11.11.11

[DOWNLOAD]
mg-cfd/1.0.0 $JARVIS_PROXY/warwick-hpsc/MG-CFD-app-OP2/archive/refs/heads/master.zip MG-CFD-app-OP2-master.zip

[DEPENDENCY]
module purge
./jarvis -install bisheng/2.1.0 com
module use ./software/modulefiles
module load bisheng/2.1.0
export CC=clang CXX=clang++ FC=flang
./jarvis -install hmpi/1.1.1 clang
module load hmpi/1.1.1
./jarvis -install scotch/7.0.1 clang+mpi
./jarvis -install parmetis/4.0.3 clang+mpi
./jarvis -install hdf5/1.10.1/clang clang+mpi
./jarvis -install kahip/3.10 clang+mpi
unzip ${JARVIS_DOWNLOAD}/MG-CFD-app-OP2-master.zip

[ENV]
module purge
module use ./software/modulefiles
module load bisheng/2.1.0
module load hmpi/1.1.1
export CC=clang CXX=clang++ FC=flang
module load scotch/7.0.1
module load parmetis/4.0.3
module load kahip/3.10
module load hdf5-clang/1.10.1
export OP2_COMPILER='clang'
export LIB_PATH=$JARVIS_LIBS/bisheng2.1.0/hmpi1.1.11
export CUDA_INSTALL_PATH=/usr/local/cuda-11.4
export MPI_INSTALL_PATH=$JARVIS_MPI/hmpi1.1.1-bisheng2.1.0/1.1.1
export PTSCOTCH_INSTALL_PATH=$LIB_PATH/scotch/7.0.1
export PARMETIS_INSTALL_PATH=$LIB_PATH/parmetis/4.0.3
export KAHIP_INSTALL_PATH=$LIB_PATH/kahip/3.10/lib/parallel
export HDF5_INSTALL_PATH=$LIB_PATH/hdf5-clang/1.10.1
export CPLUS_INCLUDE_PATH=$LIB_PATH/kahip/3.10/lib:$CPLUS_INCLUDE_PATH
export OP2_INSTALL_PATH=$JARVIS_ROOT/OP2-Common-master/op2
export CC=mpicc CXX=mpic++ FC=mpif90
export NV_ARCH={Ampere}
export PATH=/usr/local/cuda-11.4/bin:$PATH

[APP]
app_name = MG-CFD
build_dir = $JARVIS_ROOT/MG-CFD-app-OP2-master/
binary_dir = 
case_dir = $JARVIS_ROOT/MG-CFD-app-OP2-master/

[CLEAN]
./cleanOp2.sh

[BUILD]
./translate2op2.sh
make -j

[RUN]
#run = 
#binary = ./airfoil_seq

#run = mpirun --allow-run-as-root -np 8 -mca pml ucx -x UCX_NET_DEVICES=mlx5_0:1 -mca btl ^vader,tcp,openib,uct -x UCX_TLS=self,sm,rc
#binary = airfoil_mpi

#run = 
#binary = ./airfoil_genseq

#run = mpirun --allow-run-as-root -np 8 -mca pml ucx -x UCX_NET_DEVICES=mlx5_0:1 -mca btl ^vader,tcp,openib,uct -x UCX_TLS=self,sm,rc
#binary = airfoil_mpi_genseq

#run = 
#binary = export OMP_NUM_THREADS=4 && ./airfoil_openmp OP_PART_SIZE=256

run = mpirun --allow-run-as-root -np 4 -mca pml ucx -x UCX_NET_DEVICES=mlx5_0:1 -mca btl ^vader,tcp,openib,uct -x UCX_TLS=self,sm,rc -x OMP_NUM_THREADS=8
binary = airfoil_mpi_openmp OP_PART_SIZE=256
nodes = 1


