[SERVER]
11.11.11.11

[DOWNLOAD]
Amber-benchmark/20 https://ambermd.org/Amber20_Benchmark_Suite.tar.gz

[DEPENDENCY]
module use ./software/modulefiles
module purge
./jarvis -install openblas/0.3.18 gcc
./jarvis -install bisheng/2.5.0 com
module load bisheng/2.5.0
export CC=clang CXX=clang++ FC=flang
./jarvis -install hmpi/1.2.0 clang
if [ ! -d "./amber22_src" ];then
    tar -xvf $JARVIS_DOWNLOAD/Amber22.tar.bz2
    tar -xvf $JARVIS_DOWNLOAD/AmberTools22.tar.bz2
    cd amber22_src
    sed -i '22{h;d};23G' AmberTools/src/reduce/libpdb/pdb++.cpp
    sed -i '30d' AmberTools/src/reduce/libpdb/pdb++.cpp
    sed -i '24a }' AmberTools/src/reduce/libpdb/pdb++.cpp

    sed -i '23{h;d};24G' AmberTools/src/reduce/libpdb/pdb_type.cpp
    sed -i '33d' AmberTools/src/reduce/libpdb/pdb_type.cpp
    sed -i '26a }' AmberTools/src/reduce/libpdb/pdb_type.cpp

    sed -i "27s/^/!/g" AmberTools/src/sebomd/se_etimer.F
    sed -i "27a call cpu_time(tnow)" AmberTools/src/sebomd/se_etimer.F
fi
#yum install cmake* flex* bison* boost* python2-pip -y
#pip2 install numpy scipy matplotlib

[ENV]
module use ./software/modulefiles
module use ./software/moduledeps
module purge
module load bisheng/2.5.0
module load hmpi/1.2.0
module load gcc7.3.0/openblas/0.3.18
export CC=mpicc CXX=mpicxx FC=mpifort

[APP]
app_name = Amber
build_dir = $JARVIS_ROOT/amber22_src/build
#binary_dir = $JARVIS_ROOT/amber22/bin
case_dir = $JARVIS_ROOT/workloads/amber/PME/STMV_production_NPT_4fs
binary_dir = 
#case_dir = $JARVIS_ROOT/workloads/amber/PME/JAC_production_NPT_4fs

[BUILD]
sed -i "137s/gfortran/flang/g" ../cmake/AmberCompilerConfig.cmake
mpi_path=`which mpicc`
mpi_path=${mpi_path%/*/*}

content='cmake $AMBER_PREFIX/amber22_src  -DCMAKE_INSTALL_PREFIX='$JARVIS_ROOT'/amber22 -DCOMPILER=CLANG -DBUILD_PYTHON=FALSE -DMPI=TRUE -DCUDA=TRUE -DINSTALL_TESTS=TRUE -DDOWNLOAD_MINICONDA=FALSE -DMINICONDA_USE_PY3=FLASE -DCMAKE_VERBOSE_MAKEFILE=ON -DCMAKE_C_FLAGS="-O3 -mllvm -force-customized-pipeline -march=armv8.2-a -mcpu=tsv110 -Wl,-z,muldefs -fuse-ld=lld" -DCMAKE_CXX_FLAGS="-O3 -mllvm -force-customized-pipeline -march=armv8.2-a -mcpu=tsv110 -Wno-c++11-narrowing -Wl,-z,muldefs -fuse-ld=lld" -DCMAKE_Fortran_FLAGS="-O3 -mllvm -force-customized-pipeline -march=armv8.2-a -mcpu=tsv110 -I'${mpi_path}'/include -L'${mpi_path}'/lib -fuse-ld=lld"  2>&1 | tee  cmake.log' 
sed -i "40d" run_cmake 
sed -i "39a $content" run_cmake
#./run_cmake
make -j install

[CLEAN]
./clean_build

[RUN]
run = dsub -s run.sh
#run = numactl -C 0,1,2,3
#binary = pmemd.cuda -O -i mdinOPT.GPU -o mdout -p ../Topologies/STMV.prmtop -c ../Coordinates/STMV.inpcrd && cat mdout
#binary = pmemd.cuda -O -i mdinOPT.GPU -o mdout -p ../Topologies/JAC.prmtop -c ../Coordinates/JAC.inpcrd && cat mdout
binary =
nodes = 1
