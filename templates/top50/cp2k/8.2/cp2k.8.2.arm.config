[SERVER]
11.11.11.11

[DOWNLOAD]
cp2k/8.2 https://521github.com/extdomains/github.com/cp2k/cp2k/releases/download/v8.2.0/cp2k-8.2.tar.bz2 

[DEPENDENCY]
set -e
set -x
module purge
module use $JARVIS_MODULES/modules
./jarvis -install hpckit/2025.3.30 any
module load hpckit/2025.3.30
module load bisheng/compiler4.2.0/bishengmodule
export CC=clang CXX=clang++ FC=flang
./jarvis -install cmake/3.28.2 any
module load cmake/3.28.2
./jarvis -install gmp/6.2.0 clang
./jarvis -install boost/1.72.0 clang
./jarvis -install openblas/0.3.14 any
./jarvis -install spglib/1.16.0 clang
./jarvis -install libxc/5.1.4 clang
./jarvis -install gsl/2.6 clang
./jarvis -install libvori/21.04.12 clang
./jarvis -install lapack/3.8.0/bisheng clang

module load openblas/0.3.14
module load gsl/2.6-bisheng4.2.0
module load gmp/6.2.0-bisheng4.2.0
module load boost/1.72.0-bisheng4.2.0
module load lapack-bisheng/3.8.0-bisheng4.2.0
module load bisheng/hmpi25.0.0/hmpi
export CC=mpicc CXX=mpicxx FC=mpifort
./jarvis -install libint/2.6.0 clang+mpi
./jarvis -install fftw/3.3.8 clang+mpi
./jarvis -install scalapack/2.1.0/clang clang+mpi
./jarvis -install plumed/2.6.2 clang+mpi
module load scalapack-clang/2.1.0-bisheng4.2.0-hmpi25.0.0 
./jarvis -install elpa/2020.11.001 clang+mpi
cd $JARVIS_DEV
tar -jxvf $JARVIS_DOWNLOAD/cp2k-8.2.tar.bz2
# workload copy
mkdir -p $JARVIS_JOBSCRIPT/cp2k/8.2
cp -rf cp2k-8.2/benchmarks $JARVIS_JOBSCRIPT/cp2k/8.2

[ENV]
module purge
module use $JARVIS_MODULES/modules
module load hpckit/2025.3.30
module load bisheng/compiler4.2.0/bishengmodule
module load cmake/3.28.2
module load openblas/0.3.14-bisheng4.2.0
module load gsl/2.6-bisheng4.2.0
module load gmp/6.2.0-bisheng4.2.0
module load boost/1.72.0-bisheng4.2.0
module load lapack-bisheng/3.8.0-bisheng4.2.0
module load spglib/1.16.0-bisheng4.2.0
module load libxc/5.1.4-bisheng4.2.0
module load libvori/21.04.12-bisheng4.2.0

module load bisheng/hmpi25.0.0/hmpi

module load scalapack-clang/2.1.0-bisheng4.2.0-hmpi25.0.0
module load libint/2.6.0-bisheng4.2.0-hmpi25.0.0
module load fftw/3.3.8-bisheng4.2.0-hmpi25.0.0
module load plumed/2.6.2-bisheng4.2.0-hmpi25.0.0
module load elpa/2020.11.001-bisheng4.2.0-hmpi25.0.0

export CC=mpicc CXX=mpicxx FC=mpifort
export BUILD_TYPE=Linux-arm-bs
export PATH=${JARVIS_DEV}/cp2k-8.2/exe/$BUILD_TYPE:$PATH

[APP]
app_name = cp2k
app_version = 8.2
compiler = bisheng+mpi
build_dir = ${JARVIS_DEV}/cp2k-8.2
binary_dir = ${JARVIS_DEV}/cp2k-8.2/exe/$BUILD_TYPE/
case_dir = ${JARVIS_JOBSCRIPT}/cp2k/8.2/benchmarks/QS/

[BUILD]
set -e
set -x
cd ${JARVIS_DEV}/cp2k-8.2/arch
cp ${JARVIS_TPL}/top50/cp2k/8.2/$BUILD_TYPE.psmp ./
sed -i "9s%path1%${HMPI_PATH}%g" $BUILD_TYPE.psmp
sed -i "10s%path2%${BISHENG_PATH}%g" $BUILD_TYPE.psmp
cd ..
#make -j ARCH=$BUILD_TYPE VERSION=psmp
mkdir -p $1/bin
cp -rf ${JARVIS_DEV}/cp2k-8.2/exe/$BUILD_TYPE/* $1/bin

[CLEAN]
make -j 128 ARCH=$BUILD_TYPE VERSION=psmp clean

[RUN]
run = numactl -C 0-63  mpirun --allow-run-as-root -np 64 -map-by ppr:64:node:pe=1 -bind-to core -x OMP_NUM_THREADS=1 
binary = cp2k.psmp H2O-256.inp
nodes = 1

[BATCH]
#!/bin/bash

logfile=cp2k.H2O-256.inp.log

nvidia-smi -pm 1
nvidia-smi -ac 1215,1410

echo 3 > /proc/sys/vm/drop_caches
echo "===run 32C*GPU===" >> $logfile
mpirun -np 32 -genv OMP_NUM_THREADS=1 -genv CUDA_VISIBLE_DEVICES=0 exe/Linux-arm-clang-sve/cp2k.psmp benchmarks/QS/H2O-256.inp > cp2k.H2O-256.inp.log  >> $logfile 2>&1

echo 3 > /proc/sys/vm/drop_caches
echo "===run 32C*2GPU===" >> $logfile
mpirun -np 32 -genv OMP_NUM_THREADS=1 -genv CUDA_VISIBLE_DEVICES=0,1 exe/Linux-arm-clang-sve/cp2k.psmp benchmarks/QS/H2O-256.inp > cp2k.H2O-256.inp.log  >> $logfile 2>&1


echo 3 > /proc/sys/vm/drop_caches
echo "===run 64C*GPU===" >> $logfile
mpirun -np 64 -genv OMP_NUM_THREADS=1 -genv CUDA_VISIBLE_DEVICES=0 exe/Linux-arm-clang-sve/cp2k.psmp benchmarks/QS/H2O-256.inp > cp2k.H2O-256.inp.log  >> $logfile 2>&1

echo 3 > /proc/sys/vm/drop_caches
echo "===run 64C*2GPU===" >> $logfile
mpirun -np 32 -genv OMP_NUM_THREADS=1 -genv CUDA_VISIBLE_DEVICES=0,1 exe/Linux-arm-clang-sve/cp2k.psmp benchmarks/QS/H2O-256.inp > cp2k.H2O-256.inp.log  >> $logfile 2>&1


echo 3 > /proc/sys/vm/drop_caches
echo "===run 128C*GPU===" >> $logfile
mpirun -np 128 -genv OMP_NUM_THREADS=1 -genv CUDA_VISIBLE_DEVICES=0 exe/Linux-arm-clang-sve/cp2k.psmp benchmarks/QS/H2O-256.inp > cp2k.H2O-256.inp.log  >> $logfile 2>&1

echo 3 > /proc/sys/vm/drop_caches
echo "===run 128C*2GPU===" >> $logfile
mpirun -np 128 -genv OMP_NUM_THREADS=1 -genv CUDA_VISIBLE_DEVICES=0,1 exe/Linux-arm-clang-sve/cp2k.psmp benchmarks/QS/H2O-256.inp > cp2k.H2O-256.inp.log  >> $logfile 2>&1

[JOB]
#!/bin/bash
#DSUB -n cp2k
#DSUB --job_type cosched
#DSUB -N 1
#DSUB -R "cpu=128"
#DSUB -o cp2k_%J.log
#DSUB -e cp2k_err_%J.log
#DSUB -T '2h'

source ./env.sh # or module load cp2k
export HOSTFILE=hostfile.cp2k
rm -f $HOSTFILE
touch $HOSTFILE
cat ${CCSCHEDULER_ALLOC_FILE} | sort > $HOSTFILE
mpirun -np 128 --hostfile $HOSTFILE -np 64 -map-by ppr:64:node:pe=1 -bind-to core -x OMP_NUM_THREADS=1 cp2k.psmp H2O-256.inp
